---
title: "MPI--Hello World Example"
date: 2023-05-29
categories: [MPI, coding]
---

In this section, some basic examples will be provided to help understand how MPI works. We can first go through a "Hello World" example.

```c
#include <stdio.h>
#include <string.h>
#include <mpi.h>

int main(int argc, char ** argv[]){
	int my_rank;
	int num_of_processes;
	int source; // sender
	int dest;   // receiver
	int tag = 0;
	char message[100];
	MPI_Status status;
	
	MPI_Init(&argc, &argv);
	MPI_Comm_size(MPI_COMM_WORLD, &num_of_processes);
	MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
	if (my_rank != 0){
		sprintf(message,"Greetings from process %d",my_rank);
		dest = 0;
		MPI_Send(message, strlen(message+1),MPI_CHAR,dest,tag,MPI_COMM_WORLD);
	}else{
		for (source = 1; source < num_of_processes; source++){
			MPI_Recv(message,100,MPI_CHAR,source,tag,MPI_COMM_WORLD,&status);
			printf("%s\n", message);
		}
	}
	MPI_Finalize();
}
```

The program goes likes this:
1. The hello world program first include the MPI library. 
2. The MPI process starts with `MPI_Init` and end with `MPI_Finalize`. 
3. Inside the MPI block, different processes can use `MPI_Comm_rank` to check the current process number and use `MPI_Comm_size` to obtain the total number of processes. 
4. Inside the MPI block, one process can send or receive message from different processes

MPI uses a consistent scheme for MPI-defined identifiers, which begins with 'MPI_'. The MPI program also takes in its own datatype, as will be shown below the MPI Datatype and C equivalent Datatype.

Elementary MPI datatypes

| MPI Datatype      | C equivalent |
| ----------- | ----------- |
| MPI_SHORT      | short int       |
| MPI_INT   | int        |
|MPI_LONG | long int|
|MPI_LONG_LONG|long long int|
|MPI_UNSIGNED_CHAR|unsigned char|
|MPI_UNSIGNED_SHORT|unsigned short int|
|MPI_UNSIGNED|unsigned int|
|MPI_UNSIGNED_LONG|unsigned long int|
|MPI_UNSIGNED_LONG_LONG|unsigned long long int|
|MPI_FLOAT|float|
|MPI_DOUBLE|double|
|MPI_LONG_DOUBLE|long double|
|MPI_BYTE|char|

Next, we look at the MPI functions used in the example: 
1. `int MPI_Init(int* argc, char** argv[])` Initialize the MPI process
2. `int MPI_Finalize(void)` Finish the MPI process
3. `int MPI_Comm_Size(MPI_Comm comm, int* number_of_processes)` Find out how many processes are involved in the execution of a program.
Note that, the communicator is a collection of processes that can send messages to each other. `MPI_COMM_WORLD` is a predefined communicator which consists of all processes runnning when program execution begins
4. `int MPI_Comm_rank(MPI_Comm comm, int* my_rank)` Find out the rank of a process
5. `int MPI_Send(void* message, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm)` Send message with its length, datatype, destination and a tag.
6. `int MPI_Recv(void* message, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Status* status)` Receive message from some source with its length, datatype, destination and a tag. Receive function does not necessarily need to know the exact size of the message, but at least buffer space for receive the message. Tag here is to make sure the message won't mess up.

As can be seen from these functions, one can process data in one process and then send the message to a root process to collect all the data in each process and combine them. Sending and receiving from one process to another is often reffered as point-to-point communication.

In the next section[[2023-05-30-MPI-3]], an example of application will be used to enhance our understanding.