---
title: "MPI--Collective Communication"
date: 2023-05-30
categories: [MPI, coding]
---
In  the previous section, we showed how point-to-point communication works. In this section, we will show how the collective communication saves effort.

In the [previous section]({% post_url 2023-05-30-MPI-3 %}), when we want to read in the parameter with one node and send to every other processes, we need to send the message from the root process and receive message from other processes. This is of low efficiency as there are always some idle processes waiting for receiving message.

## Broadcast
A collective communication method appears to solve this problem. The first collective communication we want to introduce is **broadcast**. The broadcast uses a tree structure to send and receive message. 

For example, we have 8 nodes, from 0 to 7. In a tree structure, we send message as follows:
1. 0->1
2. 0->2, 1->3
3. 0->4, 1->5, 2->6, 3->7
Now that only three layers of tree is used and is of high efficiency. In MPI programming, we do not need to do this ourselves, but rather use the function `int MPI_Bcast(void* message, int count, MPI_Datatype datatype, int root, MPI_Comm comm)` directly.

The function sends the message from the root process to each processes in the communicator comm. One example is shown below:
```
MPI_Bcast(a, 1, MPI_FLOAT, 0, MPI_COMM_WORLD)
```

## Reduce
Another collective communication function is `int MPI_Reduce(void* operand, void* result, int count, MPI_Datatype datatype, MPI_Op operator, int root, MPI_Comm comm)`. Reduce function is performed on each process with some defined binary operation, which is shown below.

| Operation Name     | Meaning |
| ----------- | ----------- |
|MPI_MAX      | Maximum       |
| MPI_MIN   | Minimum        |
|MPI_SUM | Sum|
|MPI_PROD|Product|
|MPI_LAND|Logical and|
|MPI_BAND|Bitwise and|
|MPI_LOR|Logical or|
|MPI_BOR|Bitwise or|
|MPI_LXOR|Logical exclusive or|
|MPI_BXOR|Bitwise exclusive or|
|MPI_MAXLOC|Maximum and location of maximum|
|MPI_MINLOC|Minimum and location of minimum|

One example is shown here 
```
MPI_Reduce(&integral, &total, 1, MPI_FLOAT, MPI_SUM, 0, MPI_COMM_WORLD)
```

Note that one can not pass same argument to both operand and result, which is called *aliasing* of arguments.

### Application: Dot Product

Here, we provide an example using MPI_Reduce to achieve dot product.
$$
x \cdot y = x_0 y_0 + x_1y_1+...+x_{n-1}y_{n-1}
$$
A serial program can be written as:
```C
float Serial_dot(float x[], float y[], int n){
	int i;
	float sum = 0.0;
	for(i=0; i<n; i++){
		sum = sum + x[i] * y[i];
	}
	return sum;
}
```

Consider a **block** distribution of data, we partition the data so that equal amount of data is calculated on each processes, and the global result can be obtained from local results.

```C
float Parallel_dot(float local_x[], float local_y[], int n_bar){
	float local_dot;
	float dot = 0.0;
	float Serial_dot(float x[], float y[], int m);

	local_dot = Serial_dot(local_x,local_y,n_bar);
	MPI_Reduce(&local_dot, &dot, 1, MPI_FLOAT, MPI_SUM, 0, MPI_COMM_WORLD);
	return dot;
}
```

## Allreduce

In the previous application, only the root process 0 return the result while other processes will return 0. It is cool when we do not want to any further operations on other processes. However, if we do want to do some thing based on the previous result, we should call `MPI_Bcast` to pass the message. This again, is of low efficiency as we can compute all results on each processors with a so-called **butterfly** structure. 

The structure can be described as:
1. The processes are divided into pairs. Each pair exchange local results within the pair
2. Each process adds together the results
3. Each pair exchange result with other pairs
4. Each process adds together the results
5. continue exchanging with not added pairs

This can be easily achieved by function `int MPI_Allreduce(void* operand, void* result, int count, MPI_Datatype datatype, MPI_Op operator, MPI_Comm comm)`

## Scatter and Gather

Similar to `MPI_Bcast`, `MPI_Scatetr` is designed to send chunks of an array to different processes. Let's take a closer look at the function. 
```
MPI_Scatter(
    void* send_data,
    int send_count,
    MPI_Datatype send_datatype,
    void* recv_data,
    int recv_count,
    MPI_Datatype recv_datatype,
    int root,
    MPI_Comm communicator)
```
The first parameter, `send_data`, is an array of data that resides on the root process. The second and third parameters, `send_count` and `send_datatype`, dictate how many elements of a specific MPI Datatype will be sent to each process. If `send_count` is one and `send_datatype` is `MPI_INT`, then process zero gets the first integer of the array, process one gets the second integer, and so on. If `send_count` is two, then process zero gets the first and second integers, process one gets the third and fourth, and so on. In practice, `send_count` is often equal to the number of elements in the array divided by the number of processes.

`MPI_Gather` is the inverse of `MPI_Scatter`, which takes elements from many processes and gather them to one single process. The details can be found below.
```
MPI_Gather(
    void* send_data,
    int send_count,
    MPI_Datatype send_datatype,
    void* recv_data,
    int recv_count,
    MPI_Datatype recv_datatype,
    int root,
    MPI_Comm communicator)
```

## Allgather
Similar to the previously discussed, the gather function can be performed in a butterfly fashion to gather information to each process and save time. 
```
MPI_Allgather(
    void* send_data,
    int send_count,
    MPI_Datatype send_datatype,
    void* recv_data,
    int recv_count,
    MPI_Datatype recv_datatype,
    MPI_Comm communicator)
```


Now, consider an example of matrix-vector multiplication
$$
y_k=a_{k,0}x_0+a_{k,1}x_1+...+a_{k,n-1}x_{n-1}
$$
Again, we will first provide a serial version
```C
void Serial_matrix_vector_prod(MATRIX_T A, int m, int n, float x[], float y[]){
	int k,j;
	for(k=0;k<m;k++){
		y[k] = 0.0;
		for(j=0; j<n; j++){
			y[k] = y[k] +A[k][j] * x[j];
		}
	}
}
```

To consider the parallelize version, we need to consider how the data can be partitioned to each process. One of the simplest method for matrix distribution is a **block-row** or **panel** distribution. In this distribution, the matrix is partitioned by the rows. For example, consider a matrix with 8 rows and distributed on 4 processes. Each process will take charge of 2 rows. 

```C
void Parallel_matrix_vector_prod(MATRIX_T local_A, int local_m, int n, float x, float local_y[], float global_y[]){
	int i,j;
	for(i=0; i<local_m; i++){
		local_y[i] = 0.0;
		for (j=0;j<n;j++){
			local_y[i] = local_y[i] + local_A[i][j]*x[j];
		}
	}
	MPI_Allgather(local_y, n, MPI_FLOAT, global_y, n, MPI_FLOAT, MPI_COMM_WORLD);
}
```

## Alltoall
`MPI_Alltoall` is a combination of `MPI_Scatter` and `MPI_Scatter`. That is, every process has a buffer containing elements that will be scattered across all processes, as well as a buffer in which store elements that will be gathered from all other processes.
```C
int MPI_Alltoall(const void* buffer_send,
				 int count_send,
				 MPI_Datatype datatype_send,
				 void* buffer_recv,
				 int count_recv,
				 MPI_Datatype datatype_recv,
				 MPI_Comm communicator);
```

`