---
title: "MPI--Collective Communication"
date: 2023-05-30
categories: [MPI, coding]
use:math
---
In  the previous section, we showed how point-to-point communication works. In this section, we will show how the collective communication saves effort.

In the [previous section]({% post_url 2023-05-30-MPI-3 %}), when we want to read in the parameter with one node and send to every other processes, we need to send the message from the root process and receive message from other processes. This is of low efficiency as there are always some idle processes waiting for receiving message.

A collective communication method appears to solve this problem. The first collective communication we want to introduce is **broadcast**. The broadcast uses a tree structure to send and receive message. 

For example, we have 8 nodes, from 0 to 7. In a tree structure, we send message as follows:
1. 0->1
2. 0->2, 1->3
3. 0->4, 1->5, 2->6, 3->7
Now that only three layers of tree is used and is of high efficiency. In MPI programming, we do not need to do this ourselves, but rather use the function `int MPI_Bcast(void* message, int count, MPI_Datatype datatype, int root, MPI_Comm comm)` directly.

The function sends the message from the root process to each processes in the communicator comm. 
